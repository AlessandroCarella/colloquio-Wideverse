{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score,  balanced_accuracy_score,\n",
    "                             roc_curve, auc, confusion_matrix, classification_report)\n",
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "# !pip install catboost\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, \n",
    "                             r2_score, mean_absolute_percentage_error)\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy import stats\n",
    "\n",
    "dfClassBalancedFile = \"data/df_clean_encoded_train_no_outliers_scaled_classification_balanced.csv\"\n",
    "dfClassImBalancedFile = \"data/df_clean_encoded_train_no_outliers_scaled_classification.csv\"\n",
    "dfRegImBalancedFile = \"data/df_clean_encoded_train_no_outliers_scaled_regression.csv\"\n",
    "\n",
    "dfClassValFile = \"data/df_clean_encoded_validation_classification.csv\"\n",
    "dfClassTestFile = \"data/df_clean_encoded_test_classification.csv\"\n",
    "dfRegValFile = \"data/df_clean_encoded_validation_regression.csv\"\n",
    "dfRegTestFile = \"data/df_clean_encoded_test_regression.csv\"\n",
    "\n",
    "TARGET_COL = 'vote_average'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae601c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso 3/4 dei core totali del pc per parallelizzare i processi\n",
    "total_cores = mp.cpu_count()\n",
    "n_jobs = int(total_cores * 3 / 4)\n",
    "print(f\"Total cores available: {total_cores}\")\n",
    "print(f\"Using {n_jobs} cores for parallel processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02773a19",
   "metadata": {},
   "source": [
    "### classificazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f280c4c",
   "metadata": {},
   "source": [
    "l'approccio che voglio prendere è provare molti classificatori, con parameter grids ampie così da vedere quali di loro, a grandi linee performa meglio\n",
    "\n",
    "una volta trovato il classificatore \"giusto\" vado ad esplorare meglio i parametri di quello così da ottimizzare le performance\n",
    "\n",
    "ovviamente utilizzerò k-fold validation, a 5 per i primi test, e a 20 per i test più granulari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e97a2e",
   "metadata": {},
   "source": [
    "##### parametri e funzioni comuni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grids con ~9 combinazioni per ogni classificatore\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2]\n",
    "    },\n",
    "    \n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    \n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    \n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5]\n",
    "    },\n",
    "    \n",
    "    'LogisticRegression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    \n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 50],\n",
    "        'max_depth': [-1]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 6]\n",
    "    },\n",
    "    \n",
    "    'CatBoost': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'depth': [4, 6]\n",
    "    },\n",
    "    \n",
    "    'BernoulliNB': {\n",
    "        'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "        'binarize': [0.0, 0.5]\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0),\n",
    "    'BernoulliNB': BernoulliNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifiers(X_train, y_train, X_val, y_val, param_grids, models, n_jobs, cv_folds=5):\n",
    "    \"\"\"Train multiple classifiers with GridSearchCV\"\"\"\n",
    "    results = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    print(f\"Inizio Grid Search con {cv_folds}-Fold CV su {n_jobs} cores\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] training {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grids[model_name],\n",
    "                cv=cv_folds,\n",
    "                n_jobs=n_jobs,\n",
    "                scoring='f1_macro',\n",
    "                verbose=1,\n",
    "                return_train_score=True\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            best_models[model_name] = grid_search.best_estimator_\n",
    "            \n",
    "            train_pred = grid_search.predict(X_train)\n",
    "            val_pred = grid_search.predict(X_val)\n",
    "            \n",
    "            try:\n",
    "                train_pred_proba = grid_search.predict_proba(X_train)\n",
    "                val_pred_proba = grid_search.predict_proba(X_val)\n",
    "                has_proba = True\n",
    "            except:\n",
    "                has_proba = False\n",
    "            \n",
    "            def calculate_metrics(y_true, y_pred, y_pred_proba=None, set_name=''):\n",
    "                metrics = {\n",
    "                    f'{set_name}_accuracy': accuracy_score(y_true, y_pred),\n",
    "                    f'{set_name}_precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "                    f'{set_name}_recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "                    f'{set_name}_f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "                    f'{set_name}_balanced_acc': balanced_accuracy_score(y_true, y_pred),\n",
    "                }\n",
    "                \n",
    "                if y_pred_proba is not None:\n",
    "                    try:\n",
    "                        if len(np.unique(y_true)) == 2:\n",
    "                            metrics[f'{set_name}_roc_auc'] = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
    "                        else:\n",
    "                            metrics[f'{set_name}_roc_auc'] = roc_auc_score(y_true, y_pred_proba, \n",
    "                                                                           multi_class='ovr', average='weighted')\n",
    "                    except:\n",
    "                        metrics[f'{set_name}_roc_auc'] = np.nan\n",
    "                else:\n",
    "                    metrics[f'{set_name}_roc_auc'] = np.nan\n",
    "                \n",
    "                return metrics\n",
    "            \n",
    "            model_results = {\n",
    "                'model_name': model_name,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'cv_mean_score': grid_search.best_score_,\n",
    "                'cv_std_score': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "            }\n",
    "            \n",
    "            model_results.update(calculate_metrics(y_train, train_pred, \n",
    "                                                   train_pred_proba if has_proba else None, 'train'))\n",
    "            model_results.update(calculate_metrics(y_val, val_pred, \n",
    "                                                   val_pred_proba if has_proba else None, 'val'))\n",
    "            \n",
    "            results[model_name] = model_results\n",
    "            \n",
    "            print(f\"Miglior CV Score: {grid_search.best_score_:.4f}\")\n",
    "            print(f\"Validation Accuracy: {model_results['val_accuracy']:.4f}\")\n",
    "            print(f\"Validation Precision: {model_results['val_precision']:.4f}\")\n",
    "            print(f\"Validation Recall: {model_results['val_recall']:.4f}\")\n",
    "            print(f\"Validation F1: {model_results['val_f1']:.4f}\")\n",
    "            print(f\"Validation Balanced accuracy: {model_results['val_balanced_acc']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Tutti i modelli sono stati allenati!\")\n",
    "    \n",
    "    # Crea DataFrame con risultati\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df = results_df.sort_values('val_recall', ascending=False)\n",
    "    \n",
    "    print(\"\\nCONFRONTO MODELLI SU VALIDATION SET\")\n",
    "    summary_cols = ['cv_mean_score', 'cv_std_score',\n",
    "                    'train_accuracy', 'train_recall', 'train_f1',\n",
    "                    'val_accuracy', 'val_recall', 'val_f1',\n",
    "                    'val_precision', 'val_balanced_acc', 'val_roc_auc']\n",
    "    \n",
    "    summary_table = results_df[summary_cols].round(4)\n",
    "    print(summary_table.to_string())\n",
    "    \n",
    "    best_model_name = results_df.index[0]\n",
    "    print(f\"\\nMIGLIOR MODELLO (su Validation): {best_model_name}\")\n",
    "    \n",
    "    return results_df, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1bbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_results(results_df, top_n=10):\n",
    "    \"\"\"Crea tutti i plot per confrontare i risultati dei classificatori\"\"\"\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Plot 1-6: Comparison overview\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    top_models = results_df.head(top_n)\n",
    "    \n",
    "    # 1. Validation Metrics Comparison\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    metrics_to_plot = ['val_accuracy', 'val_precision', 'val_recall', 'val_f1', 'val_balanced_acc']\n",
    "    \n",
    "    x = np.arange(len(top_models))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        offset = width * (i - 2)\n",
    "        ax1.bar(x + offset, top_models[metric], width, \n",
    "                label=metric.replace('val_', '').replace('_', ' ').title())\n",
    "    \n",
    "    ax1.set_xlabel('Models', fontsize=10)\n",
    "    ax1.set_ylabel('Score', fontsize=10)\n",
    "    ax1.set_title('Top 10 Models - Validation Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(top_models.index, rotation=45, ha='right', fontsize=8)\n",
    "    ax1.legend(fontsize=8)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Train vs Validation F1 Score\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    x_pos = np.arange(len(top_models))\n",
    "    ax2.barh(x_pos - 0.2, top_models['train_f1'], 0.4, label='Train F1', alpha=0.8)\n",
    "    ax2.barh(x_pos + 0.2, top_models['val_f1'], 0.4, label='Validation F1', alpha=0.8)\n",
    "    ax2.set_yticks(x_pos)\n",
    "    ax2.set_yticklabels(top_models.index, fontsize=8)\n",
    "    ax2.set_xlabel('F1 Score', fontsize=10)\n",
    "    ax2.set_title('Train vs Validation F1 Score\\n(Overfitting Check)', fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. Cross-Validation Scores\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    ax3.errorbar(range(len(top_models)), top_models['cv_mean_score'], \n",
    "                 yerr=top_models['cv_std_score'], fmt='o-', capsize=5, \n",
    "                 capthick=2, markersize=8, linewidth=2)\n",
    "    ax3.set_xticks(range(len(top_models)))\n",
    "    ax3.set_xticklabels(top_models.index, rotation=45, ha='right', fontsize=8)\n",
    "    ax3.set_ylabel('CV Score', fontsize=10)\n",
    "    ax3.set_xlabel('Models', fontsize=10)\n",
    "    ax3.set_title('Cross-Validation Scores (Mean ± Std)', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Heatmap\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    heatmap_metrics = ['cv_mean_score', 'train_f1', 'val_accuracy', \n",
    "                       'val_precision', 'val_recall', 'val_f1', 'val_balanced_acc']\n",
    "    \n",
    "    heatmap_data = top_models[heatmap_metrics].apply(pd.to_numeric, errors='coerce').T\n",
    "    heatmap_data = heatmap_data.fillna(0)\n",
    "    \n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu', \n",
    "                cbar_kws={'label': 'Score'}, ax=ax4, linewidths=0.5)\n",
    "    ax4.set_title('Performance Heatmap - All Metrics', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xlabel('Models', fontsize=10)\n",
    "    ax4.set_yticklabels([m.replace('_', ' ').title() for m in heatmap_metrics], \n",
    "                         rotation=0, fontsize=8)\n",
    "    ax4.set_xticklabels(top_models.index, rotation=45, ha='right', fontsize=8)\n",
    "    \n",
    "    # 5. Overfitting Analysis\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    overfitting_gap = top_models['train_f1'] - top_models['val_f1']\n",
    "    colors = ['red' if gap > 0.1 else 'orange' if gap > 0.05 else 'green' \n",
    "              for gap in overfitting_gap]\n",
    "    bars = ax5.barh(range(len(top_models)), overfitting_gap, color=colors, alpha=0.7)\n",
    "    ax5.set_yticks(range(len(top_models)))\n",
    "    ax5.set_yticklabels(top_models.index, fontsize=8)\n",
    "    ax5.set_xlabel('Train F1 - Validation F1', fontsize=10)\n",
    "    ax5.set_title('Overfitting Analysis\\n(Green=Good, Orange=Moderate, Red=High)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax5.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax5.axvline(x=0.05, color='orange', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    ax5.axvline(x=0.1, color='red', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 6. ROC-AUC Comparison\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    roc_valid = top_models[['train_roc_auc', 'val_roc_auc']].apply(pd.to_numeric, errors='coerce')\n",
    "    roc_valid = roc_valid.dropna()\n",
    "    \n",
    "    if not roc_valid.empty:\n",
    "        x_pos = np.arange(len(roc_valid))\n",
    "        width = 0.35\n",
    "        ax6.bar(x_pos - width/2, roc_valid['train_roc_auc'], width, \n",
    "                label='Train ROC-AUC', alpha=0.8)\n",
    "        ax6.bar(x_pos + width/2, roc_valid['val_roc_auc'], width, \n",
    "                label='Validation ROC-AUC', alpha=0.8)\n",
    "        ax6.set_xticks(x_pos)\n",
    "        ax6.set_xticklabels(roc_valid.index, rotation=45, ha='right', fontsize=8)\n",
    "        ax6.set_ylabel('ROC-AUC Score', fontsize=10)\n",
    "        ax6.set_title('ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
    "        ax6.legend()\n",
    "        ax6.grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'ROC-AUC data not available', \n",
    "                 ha='center', va='center', fontsize=12)\n",
    "        ax6.set_title('ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
    "        ax6.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 7: Metriche dettagliate\n",
    "    fig3, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics = ['val_accuracy', 'val_precision', 'val_recall', \n",
    "               'val_f1', 'val_balanced_acc', 'cv_mean_score']\n",
    "    titles = ['Validation Accuracy', 'Validation Precision', 'Validation Recall',\n",
    "              'Validation F1 Score', 'Validation Balanced Accuracy', 'CV Mean Score']\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        metric_data = pd.to_numeric(results_df[metric], errors='coerce')\n",
    "        sorted_data = results_df.copy()\n",
    "        sorted_data[metric] = metric_data\n",
    "        sorted_data = sorted_data.dropna(subset=[metric])\n",
    "        sorted_data = sorted_data.sort_values(metric, ascending=True)\n",
    "        \n",
    "        values = sorted_data[metric].values\n",
    "        \n",
    "        if values.max() - values.min() > 0:\n",
    "            normalized = (values - values.min()) / (values.max() - values.min())\n",
    "        else:\n",
    "            normalized = np.ones_like(values) * 0.5\n",
    "        \n",
    "        colors_map = plt.cm.RdYlGn(normalized)\n",
    "        \n",
    "        axes[idx].barh(range(len(sorted_data)), values, color=colors_map)\n",
    "        axes[idx].set_yticks(range(len(sorted_data)))\n",
    "        axes[idx].set_yticklabels(sorted_data.index, fontsize=9)\n",
    "        axes[idx].set_xlabel('Score', fontsize=10)\n",
    "        axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "        axes[idx].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(values):\n",
    "            axes[idx].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiclass_roc_detailed(model, model_name, X, y, classes, title_suffix=''):\n",
    "    \"\"\"Plot delle curve ROC dettagliate per classificazione multiclasse\"\"\"\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X)\n",
    "    y_bin = label_binarize(y, classes=classes)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # Calcola curva ROC e area per ogni classe\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Calcola micro-average\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_pred_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Calcola macro-average\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    \n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', \n",
    "                    'purple', 'brown', 'pink', 'gray', 'olive'])\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'Class {classes[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})',\n",
    "            color='deeppink', linestyle=':', linewidth=3)\n",
    "    \n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "            label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.3f})',\n",
    "            color='navy', linestyle=':', linewidth=3)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'{model_name} - ROC Curves by Class {title_suffix}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33269814",
   "metadata": {},
   "source": [
    "#### dataset bilanciato prima analisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d260b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dfClassBalancedFile)\n",
    "dfVal = pd.read_csv(dfClassValFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=[TARGET_COL])\n",
    "y_train = df[TARGET_COL]\n",
    "\n",
    "X_val = dfVal.drop(columns=[TARGET_COL])\n",
    "y_val = dfVal[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui training\n",
    "results_df, best_models = train_classifiers(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val, \n",
    "    param_grids, \n",
    "    models, \n",
    "    n_jobs,\n",
    "    cv_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88994dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea i plot\n",
    "plot_classification_results(results_df, top_n=10)\n",
    "\n",
    "# Plot ROC per il miglior modello\n",
    "classes = np.unique(y_val)\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "fig = plot_multiclass_roc_detailed(\n",
    "    best_model, \n",
    "    best_model_name, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    classes, \n",
    "    '(Validation Set)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48679e6a",
   "metadata": {},
   "source": [
    "#### dataset non bilanciato prima analisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dfClassImBalancedFile)\n",
    "dfVal = pd.read_csv(dfClassValFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=[TARGET_COL])\n",
    "y_train = df[TARGET_COL]\n",
    "\n",
    "X_val = dfVal.drop(columns=[TARGET_COL])\n",
    "y_val = dfVal[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b089e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui training\n",
    "results_df, best_models = train_classifiers(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val, \n",
    "    param_grids, \n",
    "    models, \n",
    "    n_jobs,\n",
    "    cv_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea i plot\n",
    "plot_classification_results(results_df, top_n=10)\n",
    "\n",
    "# Plot ROC per il miglior modello\n",
    "classes = np.unique(y_val)\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "fig = plot_multiclass_roc_detailed(\n",
    "    best_model, \n",
    "    best_model_name, \n",
    "    X_val, \n",
    "    y_val, \n",
    "    classes, \n",
    "    '(Validation Set)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595c848",
   "metadata": {},
   "source": [
    "### riassunto e fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56423e70",
   "metadata": {},
   "source": [
    "come visto dagli output e grafici sopra riportati il dataset non bilanciato ottiene performance migliori sul set di validazione rispetto a quello bilanciato\n",
    "\n",
    "inoltre LightGBM, modello che performa meglio per il dataset non bilanciato, ottiene delle metriche più alte sia per quanto riguarda accuracy, recall,... sia per la metrica riguardante l'overfitting rispetto a CatBoost (modello meglio performante sul dataset bilanciato)\n",
    "\n",
    "proseguo quindi con il fine tuning del modello sul dataset non bilanciato, sapendo inoltre di, probabilmente, non avere problemi per quanto riguarda il regressore che andrò ad implementare successivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dfClassImBalancedFile)\n",
    "dfVal = pd.read_csv(dfClassValFile)\n",
    "\n",
    "X_train = df.drop(columns=[TARGET_COL])\n",
    "y_train = df[TARGET_COL]\n",
    "\n",
    "X_val = dfVal.drop(columns=[TARGET_COL])\n",
    "y_val = dfVal[TARGET_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344c63f",
   "metadata": {},
   "source": [
    "siccome nella funzione di prima non ho salvato la combinazione di parametri migliori per non affollare l'ouptut rialleno il modello con la parametr grid precedente per trovare i parametri dei test precedenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d429e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTesting {np.prod([len(v) for v in initial_param_grid.values()])} parameter combinations\")\n",
    "\n",
    "initial_grid_search = GridSearchCV(\n",
    "    estimator=lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "    param_grid=initial_param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring='f1_macro',\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting initial grid search...\")\n",
    "initial_grid_search.fit(X_train, y_train)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Initial grid search completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2818a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best CV Score: {initial_grid_search.best_score_:.4f}\")\n",
    "print(f\"Best Parameters: {initial_grid_search.best_params_}\")\n",
    "\n",
    "initial_best_model = initial_grid_search.best_estimator_\n",
    "val_pred = initial_best_model.predict(X_val)\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_val, val_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_val, val_pred, average='weighted'):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_val, val_pred, average='weighted'):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_val, val_pred, average='weighted'):.4f}\")\n",
    "print(f\"  Balanced Accuracy: {balanced_accuracy_score(y_val, val_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bf73a",
   "metadata": {},
   "source": [
    "vado poi a creare una nuova parametr grid con un po' di rumore sui parametri migliori trovati ed estendendo i parametri che imposto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f80d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_best_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae7726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightgbm.readthedocs.io/en/stable/Parameters.html\n",
    "fine_tune_param_grid_light = {\n",
    "    'n_estimators': [\n",
    "        200, 150, 250\n",
    "    ],\n",
    "    'learning_rate': [\n",
    "        0.01, 0.005, 0.015\n",
    "    ],\n",
    "    'num_leaves': [\n",
    "        50, 30, 70\n",
    "    ],\n",
    "    'max_depth': [-1, 10, 30],\n",
    "    # ho deciso di rimuovere gli altri parametri in quanto ci avrebbe messo troppo tempo\n",
    "    # 'min_child_samples': [20, 10, 30],\n",
    "    # 'subsample': [1.0, 0.8],\n",
    "    # 'colsample_bytree': [1.0, 0.8],\n",
    "    # 'reg_alpha': [0, 0.1],\n",
    "    # 'reg_lambda': [0, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "serializationFile = \"output/classification_fine_tune_grid_search_light.pkl\"\n",
    "if not os.path.isfile(serializationFile):\n",
    "    fine_tune_grid_search_light = GridSearchCV(\n",
    "        estimator=lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "        param_grid=fine_tune_param_grid_light,\n",
    "        cv=10,\n",
    "        n_jobs=n_jobs,\n",
    "        scoring='f1_macro',\n",
    "        verbose=4,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting light fine-tuning...\")\n",
    "    fine_tune_grid_search_light.fit(X_train, y_train)\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Light fine-tuning completed!\")\n",
    "    with open (serializationFile, \"wb\") as f:\n",
    "        pickle.dump(fine_tune_grid_search_light, f)\n",
    "        \n",
    "with open (serializationFile, \"rb\") as f:\n",
    "    fine_tune_grid_search_light = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fba55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best CV Score: {fine_tune_grid_search_light.best_score_:.4f}\")\n",
    "print(f\"Best Parameters: {fine_tune_grid_search_light.best_params_}\")\n",
    "\n",
    "fine_tuned_model = fine_tune_grid_search_light.best_estimator_\n",
    "val_pred_ft = fine_tuned_model.predict(X_val)\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_val, val_pred_ft):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_val, val_pred_ft, average='weighted'):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_val, val_pred_ft, average='weighted'):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_val, val_pred_ft, average='weighted'):.4f}\")\n",
    "print(f\"  Balanced Accuracy: {balanced_accuracy_score(y_val, val_pred_ft):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b773e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    'Model': ['Initial', 'Light Fine-Tuned'],\n",
    "    'CV Score': [\n",
    "        initial_grid_search.best_score_,\n",
    "        fine_tune_grid_search_light.best_score_,\n",
    "    ],\n",
    "    'Val F1': [\n",
    "        f1_score(y_val, val_pred, average='weighted'),\n",
    "        f1_score(y_val, val_pred_ft, average='weighted'),\n",
    "    ],\n",
    "    'Val Accuracy': [\n",
    "        accuracy_score(y_val, val_pred),\n",
    "        accuracy_score(y_val, val_pred_ft),\n",
    "    ],\n",
    "    'Val Recall': [\n",
    "        recall_score(y_val, val_pred, average='weighted'),\n",
    "        recall_score(y_val, val_pred_ft, average='weighted'),\n",
    "    ],\n",
    "    'Val Precision': [\n",
    "        precision_score(y_val, val_pred, average='weighted'),\n",
    "        precision_score(y_val, val_pred_ft, average='weighted'),\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9f8fc",
   "metadata": {},
   "source": [
    "come è possibile vedere dall'output precedente i risultati sono migliorati ma di molto poco, non essendo totalmente necessario per il task dato trovare i parametri migliori in assoluto decido di fermarmi qui avendo i parametri migliori per il dataset volto alla classificazione:\n",
    "* modello: lightgbm, con parametri: \n",
    "    * 'learning_rate': 0.015, \n",
    "    * 'max_depth': 10, \n",
    "    * 'n_estimators': 250, \n",
    "    * 'num_leaves': 70\n",
    "\n",
    "eventualmente avrei potuto continuare la ricerca dei parametri migliori aggiornando il dizionario fine_tune_param_grid_light e cercando i nuovi parametri in base ai risultati ottenuti, la parameter grid al prossimo step sarebbe potuta essere quella riportata qui sotto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightgbm.readthedocs.io/en/stable/Parameters.html\n",
    "fine_tune_param_grid_light = {\n",
    "    'n_estimators': [\n",
    "        250, 300, 350\n",
    "    ],\n",
    "    'learning_rate': [\n",
    "        0.015, 0.0225, 0.03,\n",
    "    ],\n",
    "    'num_leaves': [\n",
    "        70, 100, 130\n",
    "    ],\n",
    "    'max_depth': [7, 10, 13],\n",
    "    # lasciando gli altri parametri sempre commentati così da non avere una computazione troppo lunga\n",
    "    # 'min_child_samples': [20, 10, 30],\n",
    "    # 'subsample': [1.0, 0.8],\n",
    "    # 'colsample_bytree': [1.0, 0.8],\n",
    "    # 'reg_alpha': [0, 0.1],\n",
    "    # 'reg_lambda': [0, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bec95f",
   "metadata": {},
   "source": [
    "l'aggiornamento sopra riportato segue questa logica:\n",
    "* sia n_estimators che learning_rate che num_leaves hanno preso il valore all'estremo destro quindi aumento i valori di questi, sempre mantenendo gli intervalli precedentemente definiti (e.g. +-50 per n_estimators)\n",
    "* max_depth invece ha trovato il parametro migliore in mezzo alla lista iniziale ([-1, 10, 30] (per quanto -1 indichi l'assenza di un limite per la profondità degli alberi)), quindi cerco il valore migliore intorno a quello (aggiungendo e rimuovendo 3 di profondità)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f540e",
   "metadata": {},
   "source": [
    "#### training modello finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dfClassImBalancedFile)\n",
    "dfTest = pd.read_csv(dfClassTestFile)\n",
    "\n",
    "X_train = df.drop(columns=[TARGET_COL])\n",
    "y_train = df[TARGET_COL]\n",
    "\n",
    "X_test = dfTest.drop(columns=[TARGET_COL])\n",
    "y_test = dfTest[TARGET_COL]\n",
    "\n",
    "clf = lgb.LGBMClassifier(learning_rate=0.015, max_depth=10, n_estimators=250, num_leaves=70, random_state=42, verbose=-1)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd176c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_proba_test = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST SET METRICS - FINAL MODEL (LightGBM)\")\n",
    "\n",
    "test_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_test),\n",
    "    'Precision (weighted)': precision_score(y_test, y_pred_test, average='weighted', zero_division=0),\n",
    "    'Recall (weighted)': recall_score(y_test, y_pred_test, average='weighted', zero_division=0),\n",
    "    'F1 Score (weighted)': f1_score(y_test, y_pred_test, average='weighted', zero_division=0),\n",
    "    'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_test),\n",
    "}\n",
    "\n",
    "# ROC-AUC\n",
    "\n",
    "test_metrics['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba_test, \n",
    "                                            multi_class='ovr', average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric:.<40} {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f11216",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confusion Matrix (counts)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Confusion Matrix - Test Set\\n(Absolute Counts)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Confusion Matrix (normalized)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "axes[1].set_title('Confusion Matrix - Test Set\\n(Normalized by True Label)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f278f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart of all metrics\n",
    "ax1 = axes[0]\n",
    "metrics_names = list(test_metrics.keys())\n",
    "metrics_values = list(test_metrics.values())\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(metrics_names)))\n",
    "bars = ax1.barh(metrics_names, metrics_values, color=colors, alpha=0.8)\n",
    "\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Test Set Performance Metrics\\nLightGBM Classifier', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, value) in enumerate(zip(bars, metrics_values)):\n",
    "    if not np.isnan(value):\n",
    "        ax1.text(value + 0.02, i, f'{value:.4f}', \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Per-class metrics\n",
    "ax2 = axes[1]\n",
    "classes = np.unique(y_test)\n",
    "\n",
    "precision_per_class = precision_score(y_test, y_pred_test, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_test, y_pred_test, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(y_test, y_pred_test, average=None, zero_division=0)\n",
    "\n",
    "x_pos = np.arange(len(classes))\n",
    "width = 0.25\n",
    "\n",
    "ax2.bar(x_pos - width, precision_per_class, width, label='Precision', alpha=0.8)\n",
    "ax2.bar(x_pos, recall_per_class, width, label='Recall', alpha=0.8)\n",
    "ax2.bar(x_pos + width, f1_per_class, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(classes)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd93dab",
   "metadata": {},
   "source": [
    "interessante vedere come le performance del classificatore sul test set sia molto simile alla distribuzione del dataset iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df['vote_average'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.xlabel('Vote Average Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Vote Average Classes')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate validation predictions for comparison\n",
    "dfVal = pd.read_csv(dfClassValFile)\n",
    "X_val = dfVal.drop(columns=[TARGET_COL])\n",
    "y_val = dfVal[TARGET_COL]\n",
    "y_pred_val = clf.predict(X_val)\n",
    "\n",
    "val_metrics = {\n",
    "    'Accuracy': accuracy_score(y_val, y_pred_val),\n",
    "    'Precision': precision_score(y_val, y_pred_val, average='weighted', zero_division=0),\n",
    "    'Recall': recall_score(y_val, y_pred_val, average='weighted', zero_division=0),\n",
    "    'F1 Score': f1_score(y_val, y_pred_val, average='weighted', zero_division=0),\n",
    "    'Balanced Accuracy': balanced_accuracy_score(y_val, y_pred_val)\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Metric': list(val_metrics.keys()),\n",
    "    'Validation': list(val_metrics.values()),\n",
    "    'Test': [test_metrics['Accuracy'], test_metrics['Precision (weighted)'], \n",
    "             test_metrics['Recall (weighted)'], test_metrics['F1 Score (weighted)'],\n",
    "             test_metrics['Balanced Accuracy']]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa136d73",
   "metadata": {},
   "source": [
    "come visibile nell'output subito sopra la differenza fra le metriche sul test di validazione e quello di test sono molto simili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical feature groups\n",
    "categorical_groups = {\n",
    "    'genre_cluster': [f'genre_cluster_{i}' for i in range(6)],\n",
    "    'continent': ['continent_Africa', 'continent_Asia', 'continent_Europe', \n",
    "                  'continent_North America', 'continent_Oceania', 'continent_South America'],\n",
    "    'lang_macroarea': ['lang_macroarea_Africa', 'lang_macroarea_Eurasia', \n",
    "                       'lang_macroarea_North America', 'lang_macroarea_Papunesia', \n",
    "                       'lang_macroarea_South America']\n",
    "}\n",
    "\n",
    "# Create a mapping from individual feature to its group\n",
    "feature_to_group = {}\n",
    "for group_name, features in categorical_groups.items():\n",
    "    for feature in features:\n",
    "        feature_to_group[feature] = group_name\n",
    "\n",
    "# Aggregate feature importances\n",
    "aggregated_importance = {}\n",
    "\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    feature_name = row['feature']\n",
    "    importance = row['importance']\n",
    "    \n",
    "    # If it's a one-hot encoded feature, use the group name\n",
    "    # Otherwise, keep the original feature name\n",
    "    base_name = feature_to_group.get(feature_name, feature_name)\n",
    "    \n",
    "    if base_name in aggregated_importance:\n",
    "        aggregated_importance[base_name] += importance\n",
    "    else:\n",
    "        aggregated_importance[base_name] = importance\n",
    "\n",
    "# Create DataFrame with aggregated importances\n",
    "aggregated_df = pd.DataFrame({\n",
    "    'feature': list(aggregated_importance.keys()),\n",
    "    'importance': list(aggregated_importance.values())\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot aggregated importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 20\n",
    "top_features = aggregated_df.head(top_n)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color=colors, alpha=0.8)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Aggregated Importance', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Top {top_n} Most Important Features (Aggregated)\\nLightGBM Classifier', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42b282",
   "metadata": {},
   "source": [
    "come vediamo dal plot delle feature importance le features che risultano più importanti sono popularity, year e vote_count\n",
    "\n",
    "ho inoltre aggregato le features che erano state divise durante il one hot encoding così da vedere l'importanza delle feature categoriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL MODEL SUMMARY\")\n",
    "print(\"Model: LightGBM Classifier\")\n",
    "print(\"Parameters:\")\n",
    "for param, value in clf.get_params().items():\n",
    "    if param in ['learning_rate', 'max_depth', 'n_estimators', 'num_leaves', 'random_state']:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7483a82",
   "metadata": {},
   "source": [
    "### regressione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf6298",
   "metadata": {},
   "source": [
    "#### prima analisi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb727ea",
   "metadata": {},
   "source": [
    "##### parametri e funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42, max_iter=10000),\n",
    "    'ElasticNet': ElasticNet(random_state=42, max_iter=10000),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'LightGBM': lgb.LGBMRegressor(random_state=42, verbose=-1),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=42),\n",
    "    'CatBoost': CatBoostRegressor(random_state=42, verbose=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2]\n",
    "    },\n",
    "    \n",
    "    'SVR': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    \n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    \n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5]\n",
    "    },\n",
    "    \n",
    "    'Ridge': {\n",
    "        'alpha': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['auto', 'svd']\n",
    "    },\n",
    "    \n",
    "    'Lasso': {\n",
    "        'alpha': [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    \n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.01, 0.1, 1, 10],\n",
    "        'l1_ratio': [0.2, 0.5, 0.8]\n",
    "    },\n",
    "    \n",
    "    'KNeighbors': {\n",
    "        'n_neighbors': [3, 5, 7, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "    \n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 50],\n",
    "        'max_depth': [-1]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 6]\n",
    "    },\n",
    "    \n",
    "    'CatBoost': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'depth': [4, 6]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ceadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressors(X_train, y_train, X_val, y_val, param_grids, models, n_jobs, cv_folds=5):\n",
    "    \"\"\"Train multiple regressors with GridSearchCV\"\"\"\n",
    "    results = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    print(f\"Inizio Grid Search con {cv_folds}-Fold CV su {n_jobs} cores\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] training {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grids[model_name],\n",
    "                cv=cv_folds,\n",
    "                n_jobs=n_jobs,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                verbose=1,\n",
    "                return_train_score=True\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            best_models[model_name] = grid_search.best_estimator_\n",
    "            \n",
    "            train_pred = grid_search.predict(X_train)\n",
    "            val_pred = grid_search.predict(X_val)\n",
    "            \n",
    "            def calculate_metrics(y_true, y_pred, set_name=''):\n",
    "                mse = mean_squared_error(y_true, y_pred)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "                r2 = r2_score(y_true, y_pred)\n",
    "                \n",
    "                # MAPE - gestisce divisioni per zero\n",
    "                try:\n",
    "                    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "                except:\n",
    "                    # Calcolo manuale evitando divisione per zero\n",
    "                    mask = y_true != 0\n",
    "                    if mask.sum() > 0:\n",
    "                        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "                    else:\n",
    "                        mape = np.nan\n",
    "                \n",
    "                metrics = {\n",
    "                    f'{set_name}_mse': mse,\n",
    "                    f'{set_name}_rmse': rmse,\n",
    "                    f'{set_name}_mae': mae,\n",
    "                    f'{set_name}_r2': r2,\n",
    "                    f'{set_name}_mape': mape\n",
    "                }\n",
    "                \n",
    "                return metrics\n",
    "            \n",
    "            model_results = {\n",
    "                'model_name': model_name,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'cv_mean_score': -grid_search.best_score_,  # Negativo perché GridSearch usa neg_mse\n",
    "                'cv_std_score': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "            }\n",
    "            \n",
    "            model_results.update(calculate_metrics(y_train, train_pred, 'train'))\n",
    "            model_results.update(calculate_metrics(y_val, val_pred, 'val'))\n",
    "            \n",
    "            results[model_name] = model_results\n",
    "            \n",
    "            print(f\"Miglior CV MSE: {model_results['cv_mean_score']:.4f}\")\n",
    "            print(f\"Validation MSE: {model_results['val_mse']:.4f}\")\n",
    "            print(f\"Validation RMSE: {model_results['val_rmse']:.4f}\")\n",
    "            print(f\"Validation MAE: {model_results['val_mae']:.4f}\")\n",
    "            print(f\"Validation R²: {model_results['val_r2']:.4f}\")\n",
    "            print(f\"Validation MAPE: {model_results['val_mape']:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Tutti i modelli sono stati allenati!\")\n",
    "    \n",
    "    # Crea DataFrame con risultati\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df = results_df.sort_values('val_r2', ascending=False)  # Ordina per R² (più alto è meglio)\n",
    "    \n",
    "    print(\"\\nCONFRONTO MODELLI SU VALIDATION SET\")\n",
    "    summary_cols = ['cv_mean_score', 'cv_std_score',\n",
    "                    'train_mse', 'train_rmse', 'train_mae', 'train_r2',\n",
    "                    'val_mse', 'val_rmse', 'val_mae', 'val_r2', 'val_mape']\n",
    "    \n",
    "    summary_table = results_df[summary_cols].round(4)\n",
    "    print(summary_table.to_string())\n",
    "    \n",
    "    best_model_name = results_df.index[0]\n",
    "    print(f\"\\nMIGLIOR MODELLO (su Validation): {best_model_name}\")\n",
    "    \n",
    "    return results_df, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43627a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_results(results_df, top_n=10):\n",
    "    \"\"\"Crea tutti i plot per confrontare i risultati dei regressori\"\"\"\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    top_models = results_df.head(top_n)\n",
    "    \n",
    "    # 1. Validation Metrics Comparison\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    metrics_to_plot = ['val_mse', 'val_rmse', 'val_mae', 'val_r2']\n",
    "    \n",
    "    # Normalizza le metriche per visualizzazione (tranne R² che è già 0-1)\n",
    "    plot_data = top_models[metrics_to_plot].copy()\n",
    "    for col in ['val_mse', 'val_rmse', 'val_mae']:\n",
    "        max_val = plot_data[col].max()\n",
    "        if max_val > 0:\n",
    "            plot_data[col] = plot_data[col] / max_val\n",
    "    \n",
    "    x = np.arange(len(top_models))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        offset = width * (i - 1.5)\n",
    "        ax1.bar(x + offset, plot_data[metric], width, \n",
    "                label=metric.replace('val_', '').upper())\n",
    "    \n",
    "    ax1.set_xlabel('Models', fontsize=10)\n",
    "    ax1.set_ylabel('Normalized Score', fontsize=10)\n",
    "    ax1.set_title('Top 10 Models - Validation Metrics Comparison\\n(Normalized)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(top_models.index, rotation=45, ha='right', fontsize=8)\n",
    "    ax1.legend(fontsize=8)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Train vs Validation R² Score\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    x_pos = np.arange(len(top_models))\n",
    "    ax2.barh(x_pos - 0.2, top_models['train_r2'], 0.4, label='Train R²', alpha=0.8)\n",
    "    ax2.barh(x_pos + 0.2, top_models['val_r2'], 0.4, label='Validation R²', alpha=0.8)\n",
    "    ax2.set_yticks(x_pos)\n",
    "    ax2.set_yticklabels(top_models.index, fontsize=8)\n",
    "    ax2.set_xlabel('R² Score', fontsize=10)\n",
    "    ax2.set_title('Train vs Validation R² Score\\n(Overfitting Check)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. Cross-Validation Scores (MSE)\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    ax3.errorbar(range(len(top_models)), top_models['cv_mean_score'], \n",
    "                 yerr=top_models['cv_std_score'], fmt='o-', capsize=5, \n",
    "                 capthick=2, markersize=8, linewidth=2)\n",
    "    ax3.set_xticks(range(len(top_models)))\n",
    "    ax3.set_xticklabels(top_models.index, rotation=45, ha='right', fontsize=8)\n",
    "    ax3.set_ylabel('CV MSE', fontsize=10)\n",
    "    ax3.set_xlabel('Models', fontsize=10)\n",
    "    ax3.set_title('Cross-Validation MSE (Mean ± Std)\\n(Lower is Better)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Heatmap\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    heatmap_metrics = ['cv_mean_score', 'train_rmse', 'train_mae', 'train_r2',\n",
    "                       'val_rmse', 'val_mae', 'val_r2']\n",
    "    \n",
    "    heatmap_data = top_models[heatmap_metrics].apply(pd.to_numeric, errors='coerce').T\n",
    "    heatmap_data = heatmap_data.fillna(0)\n",
    "    \n",
    "    # Inverti le metriche di errore per il colore (più basso è meglio)\n",
    "    for metric in ['cv_mean_score', 'train_rmse', 'train_mae', 'val_rmse', 'val_mae']:\n",
    "        if metric in heatmap_data.index:\n",
    "            max_val = heatmap_data.loc[metric].max()\n",
    "            if max_val > 0:\n",
    "                heatmap_data.loc[metric] = 1 - (heatmap_data.loc[metric] / max_val)\n",
    "    \n",
    "    sns.heatmap(heatmap_data, annot=False, cmap='YlGnBu', \n",
    "                cbar_kws={'label': 'Score (normalized)'}, ax=ax4, linewidths=0.5)\n",
    "    ax4.set_title('Performance Heatmap - All Metrics', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xlabel('Models', fontsize=10)\n",
    "    ax4.set_yticklabels([m.replace('_', ' ').upper() for m in heatmap_metrics], \n",
    "                         rotation=0, fontsize=8)\n",
    "    ax4.set_xticklabels(top_models.index, rotation=45, ha='right', fontsize=8)\n",
    "    \n",
    "    # 5. Overfitting Analysis (R²)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    overfitting_gap = top_models['train_r2'] - top_models['val_r2']\n",
    "    colors = ['red' if gap > 0.15 else 'orange' if gap > 0.08 else 'green' \n",
    "              for gap in overfitting_gap]\n",
    "    bars = ax5.barh(range(len(top_models)), overfitting_gap, color=colors, alpha=0.7)\n",
    "    ax5.set_yticks(range(len(top_models)))\n",
    "    ax5.set_yticklabels(top_models.index, fontsize=8)\n",
    "    ax5.set_xlabel('Train R² - Validation R²', fontsize=10)\n",
    "    ax5.set_title('Overfitting Analysis\\n(Green=Good, Orange=Moderate, Red=High)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax5.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax5.axvline(x=0.08, color='orange', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    ax5.axvline(x=0.15, color='red', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 6. RMSE vs MAE Comparison\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    x_pos = np.arange(len(top_models))\n",
    "    width = 0.35\n",
    "    ax6.bar(x_pos - width/2, top_models['val_rmse'], width, \n",
    "            label='Validation RMSE', alpha=0.8)\n",
    "    ax6.bar(x_pos + width/2, top_models['val_mae'], width, \n",
    "            label='Validation MAE', alpha=0.8)\n",
    "    ax6.set_xticks(x_pos)\n",
    "    ax6.set_xticklabels(top_models.index, rotation=45, ha='right', fontsize=8)\n",
    "    ax6.set_ylabel('Error', fontsize=10)\n",
    "    ax6.set_title('RMSE vs MAE Comparison\\n(Lower is Better)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax6.legend()\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 7: Metriche in dettaglio\n",
    "    fig3, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics = ['val_r2', 'val_mse', 'val_rmse', \n",
    "               'val_mae', 'val_mape', 'cv_mean_score']\n",
    "    titles = ['Validation R² (Higher is Better)', 'Validation MSE (Lower is Better)', \n",
    "              'Validation RMSE (Lower is Better)', 'Validation MAE (Lower is Better)', \n",
    "              'Validation MAPE % (Lower is Better)', 'CV Mean MSE (Lower is Better)']\n",
    "    ascending_order = [False, True, True, True, True, True]  # R² più alto è meglio, altri più basso\n",
    "    \n",
    "    for idx, (metric, title, ascending) in enumerate(zip(metrics, titles, ascending_order)):\n",
    "        metric_data = pd.to_numeric(results_df[metric], errors='coerce')\n",
    "        sorted_data = results_df.copy()\n",
    "        sorted_data[metric] = metric_data\n",
    "        sorted_data = sorted_data.dropna(subset=[metric])\n",
    "        sorted_data = sorted_data.sort_values(metric, ascending=ascending)\n",
    "        \n",
    "        values = sorted_data[metric].values\n",
    "        \n",
    "        if values.max() - values.min() > 0:\n",
    "            if ascending:  # Per metriche dove più basso è meglio\n",
    "                normalized = 1 - (values - values.min()) / (values.max() - values.min())\n",
    "            else:  # Per R² dove più alto è meglio\n",
    "                normalized = (values - values.min()) / (values.max() - values.min())\n",
    "        else:\n",
    "            normalized = np.ones_like(values) * 0.5\n",
    "        \n",
    "        colors_map = plt.cm.RdYlGn(normalized)\n",
    "        \n",
    "        axes[idx].barh(range(len(sorted_data)), values, color=colors_map)\n",
    "        axes[idx].set_yticks(range(len(sorted_data)))\n",
    "        axes[idx].set_yticklabels(sorted_data.index, fontsize=9)\n",
    "        axes[idx].set_xlabel('Score', fontsize=10)\n",
    "        axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "        axes[idx].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(values):\n",
    "            axes[idx].text(v + (values.max() - values.min()) * 0.01, i, \n",
    "                          f'{v:.3f}', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_actual(model, model_name, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Plot delle predizioni vs valori reali per train, validation e test set\"\"\"\n",
    "    \n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    \n",
    "    n_plots = 2 \n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 5))\n",
    "    if n_plots == 2:\n",
    "        axes = [axes[0], axes[1]]\n",
    "    \n",
    "    # Training set\n",
    "    axes[0].scatter(y_train, train_pred, alpha=0.5, s=20)\n",
    "    axes[0].plot([y_train.min(), y_train.max()], \n",
    "                 [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "    axes[0].set_xlabel('Actual Values', fontsize=11)\n",
    "    axes[0].set_ylabel('Predicted Values', fontsize=11)\n",
    "    axes[0].set_title(f'{model_name} - Training Set\\nR² = {r2_score(y_train, train_pred):.4f}', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Validation set\n",
    "    axes[1].scatter(y_val, val_pred, alpha=0.5, s=20, color='orange')\n",
    "    axes[1].plot([y_val.min(), y_val.max()], \n",
    "                 [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "    axes[1].set_xlabel('Actual Values', fontsize=11)\n",
    "    axes[1].set_ylabel('Predicted Values', fontsize=11)\n",
    "    axes[1].set_title(f'{model_name} - Validation Set\\nR² = {r2_score(y_val, val_pred):.4f}', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(model, model_name, X_val, y_val):\n",
    "    \"\"\"Plot dei residui per analizzare la qualità delle predizioni\"\"\"\n",
    "    \n",
    "    val_pred = model.predict(X_val)\n",
    "    residuals = y_val - val_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Residuals vs Predicted\n",
    "    axes[0].scatter(val_pred, residuals, alpha=0.5, s=20)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[0].set_xlabel('Predicted Values', fontsize=11)\n",
    "    axes[0].set_ylabel('Residuals', fontsize=11)\n",
    "    axes[0].set_title(f'{model_name} - Residual Plot', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Histogram of Residuals\n",
    "    axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    axes[1].set_xlabel('Residuals', fontsize=11)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1].set_title(f'{model_name} - Residuals Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Q-Q Plot\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[2])\n",
    "    axes[2].set_title(f'{model_name} - Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcola statistiche sui residui\n",
    "    print(f\"\\n{model_name} - Residuals Statistics:\")\n",
    "    print(f\"Mean: {residuals.mean():.4f}\")\n",
    "    print(f\"Std: {residuals.std():.4f}\")\n",
    "    print(f\"Min: {residuals.min():.4f}\")\n",
    "    print(f\"Max: {residuals.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f3058",
   "metadata": {},
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9bdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(dfRegImBalancedFile)\n",
    "df_val = pd.read_csv(dfRegValFile)\n",
    "\n",
    "# 2. Separa features e target\n",
    "X_train = df_train.drop(columns=[TARGET_COL])\n",
    "y_train = df_train[TARGET_COL]\n",
    "X_val = df_val.drop(columns=[TARGET_COL])\n",
    "y_val = df_val[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, best_models = train_regressors(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    param_grids, models, n_jobs, cv_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_results(results_df, top_n=10)\n",
    "\n",
    "# 5. Analizza miglior modello\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "plot_predictions_vs_actual(\n",
    "    best_model, best_model_name,\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n",
    "\n",
    "plot_residuals(best_model, best_model_name, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986ac95",
   "metadata": {},
   "source": [
    "#### riassunto e fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e917a",
   "metadata": {},
   "source": [
    "come vediamo dai risultati ottenuti CatBoost è l'algoritmo che ottiene i risultati migliori, per quanto altri modelli (AdaBoost, GradientBoosting, XGBoost, RandomForest) gli si avvicinino molto\n",
    "\n",
    "come precedentemente effettuato procedo con il fine tuning del modello per migliorare ulteriolmente i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_param_grid = {\n",
    "    'iterations': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'depth': [4, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTesting {np.prod([len(v) for v in initial_param_grid.values()])} parameter combinations\")\n",
    "\n",
    "initial_grid_search = GridSearchCV(\n",
    "    estimator=CatBoostRegressor(random_state=42, verbose=0),\n",
    "    param_grid=initial_param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting initial grid search...\")\n",
    "initial_grid_search.fit(X_train, y_train)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Initial grid search completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0459e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best CV Score: {initial_grid_search.best_score_:.4f}\")\n",
    "print(f\"Best Parameters: {initial_grid_search.best_params_}\")\n",
    "\n",
    "initial_best_model = initial_grid_search.best_estimator_\n",
    "val_pred = initial_best_model.predict(X_val)\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"  Mean Squared Error: {mean_squared_error(y_val, val_pred):.4f}\")\n",
    "print(f\"  Root Mean Square Error: {np.sqrt(mean_squared_error(y_val, val_pred)):.4f}\")\n",
    "print(f\"  Mean Absolute Error: {mean_absolute_error(y_val, val_pred):.4f}\")\n",
    "print(f\"  R-squared: {r2_score(y_val, val_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77138d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_best_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e710ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://catboost.ai/docs/en/concepts/python-reference_catboostregressor\n",
    "# catboost ha davvero molti parametri settabili, qui mi limiterò o modificare quelli che avevo\n",
    "# precedentemente impostato ed aggiungerò solo una nuova loss_function\n",
    "fine_tune_param_grid_light = {\n",
    "    'iterations': [100, 70, 130],\n",
    "    'learning_rate': [0.1, 0.05, 0.2],\n",
    "    'depth': [6, 10, 14],\n",
    "    'loss_function': ['RMSE', 'MAPE'] # MAPE: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c13c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "serializationFile = \"output/regression_fine_tune_grid_search_light.pkl\"\n",
    "if not os.path.isfile(serializationFile):\n",
    "    fine_tune_grid_search_light = GridSearchCV(\n",
    "        estimator=CatBoostRegressor(random_state=42, verbose=0),\n",
    "        param_grid=fine_tune_param_grid_light,\n",
    "        cv=10,\n",
    "        n_jobs=n_jobs,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=4,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting light fine-tuning...\")\n",
    "    fine_tune_grid_search_light.fit(X_train, y_train)\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Light fine-tuning completed!\")\n",
    "    with open (serializationFile, \"wb\") as f:\n",
    "        pickle.dump(fine_tune_grid_search_light, f)\n",
    "\n",
    "with open (serializationFile, \"rb\") as f:\n",
    "    fine_tune_grid_search_light = pickle.load(f)\n",
    "    \n",
    "print(f\"Best CV Score: {fine_tune_grid_search_light.best_score_:.4f}\")\n",
    "print(f\"Best Parameters: {fine_tune_grid_search_light.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = fine_tune_grid_search_light.best_estimator_\n",
    "val_pred_ft = fine_tuned_model.predict(X_val)\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"  Mean Squared Error: {mean_squared_error(y_val, val_pred_ft):.4f}\")\n",
    "print(f\"  Root Mean Square Error: {np.sqrt(mean_squared_error(y_val, val_pred_ft)):.4f}\")\n",
    "print(f\"  Mean Absolute Error: {mean_absolute_error(y_val, val_pred_ft):.4f}\")\n",
    "print(f\"  R-squared: {r2_score(y_val, val_pred_ft):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    'Model': ['Initial', 'Light Fine-Tuned'],\n",
    "    'CV Score': [\n",
    "        initial_grid_search.best_score_,\n",
    "        fine_tune_grid_search_light.best_score_,\n",
    "    ],\n",
    "    'Val MSE': [\n",
    "        mean_squared_error(y_val, val_pred),\n",
    "        mean_squared_error(y_val, val_pred_ft),\n",
    "    ],\n",
    "    'Val RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_val, val_pred)),\n",
    "        np.sqrt(mean_squared_error(y_val, val_pred_ft)),\n",
    "    ],\n",
    "    'Val MAE': [\n",
    "        mean_absolute_error(y_val, val_pred),\n",
    "        mean_absolute_error(y_val, val_pred_ft),\n",
    "    ],\n",
    "    'Val R2': [\n",
    "        r2_score(y_val, val_pred),\n",
    "        r2_score(y_val, val_pred_ft),\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842f045",
   "metadata": {},
   "source": [
    "come per il caso del dataset modificato per la classificazione (ovviamente parlo della versione non bilanciata) i risultati fra una run e l'altra non sono cambiati molto\n",
    "\n",
    "la differenza importante è che il risultato qui sopra riportato indica che, per quanto i parametri risultati migliori nella prima grid search forssero presenti anche nella seconda run, il risultato migliore della seconda grid search sono parametri leggermenti diversi, che però ottengono risultati peggiori\n",
    "\n",
    "Questo è dovuto al fatto che il valore di cross validation precedente fosse impostato a 5, mentre nella seconda grid search, con i nuovi parametri a 10\n",
    "\n",
    "Ciò significa che gli stessi parametri di prima hanno raggiunto un risultato peggiore una volta che il numero di test è aumentato\n",
    "\n",
    "Per quanto i valori siano peggiorati non lo sono di molto e, in entrambi i casi, sono accettabili\n",
    "\n",
    "Procedo quindi ad allenare il regressore finale e a calcolare le meetriche sul dataset di test, utilizzando i nuovi migliori parametri trovati:\n",
    "* 'loss_function': 'RMSE',\n",
    "* 'verbose': 0,\n",
    "* 'random_state': 42,\n",
    "* 'depth': 10,\n",
    "* 'iterations': 70,\n",
    "* 'learning_rate': 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f64e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(dfRegImBalancedFile)\n",
    "df_test = pd.read_csv(dfRegTestFile)\n",
    "\n",
    "X_train = df_train.drop(columns=[TARGET_COL])\n",
    "y_train = df_train[TARGET_COL]\n",
    "\n",
    "X_test = df_test.drop(columns=[TARGET_COL])\n",
    "y_test = df_test[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07253678",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = CatBoostRegressor(\n",
    "    loss_function='RMSE',\n",
    "    depth=10,\n",
    "    iterations=70,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Training final regression model...\")\n",
    "reg.fit(X_train, y_train)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST SET METRICS - FINAL MODEL (CatBoost Regressor)\")\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# MAPE \n",
    "mask = y_test != 0\n",
    "test_mape = np.mean(np.abs((y_test[mask] - y_pred_test[mask]) / y_test[mask])) * 100\n",
    "\n",
    "test_metrics = {\n",
    "    'Mean Squared Error (MSE)': test_mse,\n",
    "    'Root Mean Squared Error (RMSE)': test_rmse,\n",
    "    'Mean Absolute Error (MAE)': test_mae,\n",
    "    'R-squared (R²)': test_r2,\n",
    "    'Mean Absolute Percentage Error (MAPE %)': test_mape\n",
    "}\n",
    "\n",
    "for metric, value in test_metrics.items():\n",
    "    if 'MAPE' in metric:\n",
    "        print(f\"{metric:.<45} {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{metric:.<45} {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7732ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "residuals_test = y_test - y_pred_test\n",
    "\n",
    "residuals_test_plot = residuals_test\n",
    "y_pred_test_resid_plot = y_pred_test\n",
    "y_test_resid_plot = y_test\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Residuals vs Predicted Values\n",
    "axes[0, 0].scatter(y_pred_test_resid_plot, residuals_test_plot, alpha=0.5, s=30, \n",
    "                    edgecolors='black', linewidth=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', lw=3)\n",
    "axes[0, 0].axhline(y=test_mae, color='g', linestyle=':', lw=2, alpha=0.5, \n",
    "                    label=f'±MAE ({test_mae:.3f})')\n",
    "axes[0, 0].axhline(y=-test_mae, color='g', linestyle=':', lw=2, alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Predicted Values', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Residuals (Actual - Predicted)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Residual Plot - Test Set', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Histogram of Residuals\n",
    "residuals_array = residuals_test.values if hasattr(residuals_test, 'values') else residuals_test\n",
    "axes[0, 1].hist(residuals_array, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--', lw=3, label='Zero Error')\n",
    "axes[0, 1].axvline(x=residuals_array.mean(), color='orange', linestyle=':', lw=2, \n",
    "                    label=f'Mean ({residuals_array.mean():.3f})')\n",
    "axes[0, 1].set_xlabel('Residuals', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Distribution of Residuals - Test Set', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Q-Q Plot\n",
    "stats.probplot(residuals_array, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot - Test Set', \n",
    "                      fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Residuals vs Actual Values\n",
    "axes[1, 1].scatter(y_test_resid_plot, residuals_test_plot, alpha=0.5, s=30, \n",
    "                    edgecolors='black', linewidth=0.5)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', lw=3)\n",
    "axes[1, 1].axhline(y=test_mae, color='g', linestyle=':', lw=2, alpha=0.5)\n",
    "axes[1, 1].axhline(y=-test_mae, color='g', linestyle=':', lw=2, alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Actual Values', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Residuals (Actual - Predicted)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Residuals vs Actual Values - Test Set', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"RESIDUAL STATISTICS - TEST SET\")\n",
    "print(f\"Mean of Residuals:............................... {residuals_array.mean():.4f}\")\n",
    "print(f\"Std Dev of Residuals:............................ {residuals_array.std():.4f}\")\n",
    "print(f\"Min Residual (Under-prediction):................. {residuals_array.min():.4f}\")\n",
    "print(f\"Max Residual (Over-prediction):.................. {residuals_array.max():.4f}\")\n",
    "print(f\"Median Absolute Residual:........................ {np.median(np.abs(residuals_array)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ad44a",
   "metadata": {},
   "source": [
    "i risultati ottenuti sono buoni, di fatti sia la media dei residuals che il valore assoluto mediano sono vicino allo zero e la standard deviation è bassa\n",
    "\n",
    "il valori di min e max redisual sono agli antipodi ma, come si vede dal secondo grafo (in alto a destra) e dal q-q plot i residual sono sempre abbastanza bassi tranne per degli errori alle code della distribuzione "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate validation predictions\n",
    "df_val = pd.read_csv(dfRegValFile)\n",
    "X_val = df_val.drop(columns=[TARGET_COL])\n",
    "y_val = df_val[TARGET_COL]\n",
    "y_pred_val = reg.predict(X_val)\n",
    "\n",
    "val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "val_r2 = r2_score(y_val, y_pred_val)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R²'],\n",
    "    'Validation': [val_mse, val_rmse, val_mae, val_r2],\n",
    "    'Test': [test_mse, test_rmse, test_mae, test_r2]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"TRAIN / VALIDATION / TEST COMPARISON\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec61fc",
   "metadata": {},
   "source": [
    "i risultati fra il validation set e il test set sono simili come lo erano per il dataset di classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical feature groups (same as before)\n",
    "categorical_groups = {\n",
    "    'genre_cluster': [f'genre_cluster_{i}' for i in range(6)],\n",
    "    'continent': ['continent_Africa', 'continent_Asia', 'continent_Europe', \n",
    "                  'continent_North America', 'continent_Oceania', 'continent_South America'],\n",
    "    'lang_macroarea': ['lang_macroarea_Africa', 'lang_macroarea_Eurasia', \n",
    "                       'lang_macroarea_North America', 'lang_macroarea_Papunesia', \n",
    "                       'lang_macroarea_South America']\n",
    "}\n",
    "\n",
    "# Create a mapping from individual feature to its group\n",
    "feature_to_group = {}\n",
    "for group_name, features in categorical_groups.items():\n",
    "    for feature in features:\n",
    "        feature_to_group[feature] = group_name\n",
    "\n",
    "# Get feature importances from regression model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Aggregate feature importances\n",
    "aggregated_importance = {}\n",
    "\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    feature_name = row['feature']\n",
    "    importance = row['importance']\n",
    "    \n",
    "    # If it's a one-hot encoded feature, use the group name\n",
    "    # Otherwise, keep the original feature name\n",
    "    base_name = feature_to_group.get(feature_name, feature_name)\n",
    "    \n",
    "    if base_name in aggregated_importance:\n",
    "        aggregated_importance[base_name] += importance\n",
    "    else:\n",
    "        aggregated_importance[base_name] = importance\n",
    "\n",
    "# Create DataFrame with aggregated importances\n",
    "aggregated_df = pd.DataFrame({\n",
    "    'feature': list(aggregated_importance.keys()),\n",
    "    'importance': list(aggregated_importance.values())\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot aggregated importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 20\n",
    "top_features = aggregated_df.head(top_n)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color=colors, alpha=0.8)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Aggregated Importance', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Top {top_n} Most Important Features (Aggregated)\\nCatBoost Regressor', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7335971",
   "metadata": {},
   "source": [
    "a differenza del grado sulla feature importance precedente nel caso del regressore la colona vote_count ha avuto un'importanza molto più alta rispetto alle altre features\n",
    "\n",
    "potrebbe essere interessante confrontare le performance di un regressore semplice che utilizza solo vote_count come feature per determinare vote_average con il regressore qui allenato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bf1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL MODEL SUMMARY - REGRESSION\")\n",
    "print(\"Model: CatBoost Regressor\")\n",
    "print(\"Parameters:\")\n",
    "for param, value in reg.get_params().items():\n",
    "    if param in ['loss_function', 'depth', 'iterations', 'learning_rate', 'random_state']:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    if 'MAPE' in metric:\n",
    "        print(f\"  {metric}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
